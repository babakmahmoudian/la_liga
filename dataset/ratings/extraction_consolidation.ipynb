{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data downloaded from http://www.elofootball.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_years = list(range(2017, 2023))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initiating a new Google Chrome window\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# The function below extracts the games of any given season of La Liga and\n",
    "# returns the output in the form of a dataframe\n",
    "def extract_games(year):\n",
    "    # The URL is shapes in the form of\n",
    "    # http://www.elofootball.com/country.php?countryiso=ESP&season=1999-2000\n",
    "    url = f\"http://www.elofootball.com/country.php?countryiso=ESP&season={year}-{year+1}\"\n",
    "    \n",
    "    # Retrieving the URL in the Chrome window\n",
    "    driver.get(url)\n",
    "    \n",
    "    # When the page loads, the table's empty and is later filled by AJAX calls.\n",
    "    # So, we wait until there is an element by the id of for instance\n",
    "    # \"tabcontainer\" available on the page before proceeding to\n",
    "    # extract the table.\n",
    "    # The availability of the said element shows that the table is fully loaded\n",
    "    # and filled.\n",
    "    element = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.ID, 'tabcontainer'))\n",
    "    )\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    # Retrieving the specific table amongst all\n",
    "    table_bodies = soup.findAll('tbody')\n",
    "    tbody = table_bodies[4]\n",
    "\n",
    "    # Within the table, only the first match in a specific date has the date column\n",
    "    # filled, the next matches in the same date do not have any data written in the\n",
    "    # date column. So, when a new date is seen, it is stored and as long as the date\n",
    "    # column is empty, the same date will be referenced.\n",
    "    last_date = ''\n",
    "    # Goint through all the tr elements (table rows)\n",
    "    all_matches = []\n",
    "    for tr in tbody.find_all('tr'):\n",
    "        # Retrieving all td elements (table data)\n",
    "        tds = tr.find_all('td')\n",
    "        # Getting the inner-text of each td element\n",
    "        match = [td.text for td in tds]\n",
    "        # As explained above, if the date column is empty, use the previous date value,\n",
    "        # otherwise, update the date value with the new one\n",
    "        if match[0] == '':\n",
    "            match[0] = last_date\n",
    "        else:\n",
    "            last_date = match[0]\n",
    "        all_matches.append(match)\n",
    "\n",
    "    result = pd.DataFrame(all_matches, columns=[\n",
    "        'date',\n",
    "        'division',\n",
    "        'unused_1',\n",
    "        'home',\n",
    "        'home_pre_rating',\n",
    "        'home_rating_delta',\n",
    "        'home_post_rating',\n",
    "        'result',\n",
    "        'prob_h',\n",
    "        'prob_d',\n",
    "        'prob_a',\n",
    "        'away',\n",
    "        'away_pre_rating',\n",
    "        'away_rating_delta',\n",
    "        'away_post_rating',\n",
    "        'unused_2'])\n",
    "\n",
    "    return result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the matches and merging them into a single dataframe\n",
    "df = pd.concat(\n",
    "    [extract_games(year) for year in league_years])\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing data for irrelevant leagues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Primera Division rel. ', 'Segunda Division ', 'Champions League ',\n",
       "       'Primera Division ', 'Europa League ', 'Copa del Rey ',\n",
       "       'Europa League q. ', 'Champions League q. ', 'Supercopa ',\n",
       "       'Supercup ', 'Europa Conf. League ', 'Europa Conf. League q. '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['division'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['division'] == 'Primera Division ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the match dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date column must be converted into DateTime64 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping redundant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The division column only contains the value 'Primera Division ', so it can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('division', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two unused columns must be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['unused_1', 'unused_2'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability columns are based on merely the difference in the ratings of the two teams; hence they are not very accurate and may be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['prob_h', 'prob_d', 'prob_a'], axis='columns')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The match result is available in other datasets, so it can be dropped here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('result', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the cleaned up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./dataset/ratings.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83d208b031a4a5a0824f5f0b47c0388aa3b94cdd004c2f68e4341ab470ede466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
